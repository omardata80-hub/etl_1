
```markdown
# ğŸ›ï¸ Shop Customer ETL Pipeline

## ğŸ“– About the Dataset
This project uses the **Shop Customer Data** dataset.  
The dataset contains **2000 customer records** with 8 attributes collected through membership cards.  

### Columns:
- `CustomerID` â€“ Unique customer ID  
- `Gender` â€“ Male/Female  
- `Age` â€“ Customerâ€™s age  
- `Annual Income ($)` â€“ Annual income in USD  
- `Spending Score (1-100)` â€“ Customer spending score (1 = low spender, 100 = high spender)  
- `Profession` â€“ Profession of the customer  
- `Work Experience` â€“ Years of work experience  
- `Family Size` â€“ Size of the family  

---

## ğŸ¯ Project Goal
The goal of this project is to demonstrate an **ETL pipeline** using **Python, Pandas, and PostgreSQL**.  

Steps:
1. **Extract** â†’ Load data from CSV into a Pandas DataFrame  
2. **Transform** â†’ Clean the data (remove duplicates, handle missing values, compute new features)  
3. **Load** â†’ Insert the transformed data into a PostgreSQL database  

---

## ğŸ› ï¸ Tools & Libraries
- **Python**: pandas, sqlalchemy, psycopg2  
- **PostgreSQL**: Database  
- **Jupyter Notebook**: For running the ETL steps  
- **Git & GitHub**: Version control  

---

## ğŸ“‚ Project Structure
```

etl_1/
â”‚â”€â”€ etl.ipynb         # Jupyter Notebook with ETL steps
â”‚â”€â”€ data.csv          # Input dataset (CSV file)
â”‚â”€â”€ requirements.txt  # Python dependencies
â”‚â”€â”€ README.md         # Documentation

````

---

## â–¶ï¸ How to Run
1. Clone the repo:
   ```bash
   git clone https://github.com/omardata80-hub/etl_1.git
   cd etl_1
````

2. Install dependencies:

   ```bash
   pip install -r requirements.txt
   ```

3. Run the ETL notebook:

   ```bash
   jupyter notebook etl.ipynb
   ```

4. Verify the data in PostgreSQL:

   ```sql
   SELECT * FROM customers LIMIT 10;
   ```

---

## âœ… Deliverables

* Working ETL pipeline (`etl.ipynb`)
* PostgreSQL database with loaded data
* Documentation (`README.md`)

`
